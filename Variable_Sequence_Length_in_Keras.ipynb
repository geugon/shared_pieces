{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras enables models to use variable-size inputs by using None as a dimension in the input space.  Less clearly documented, this will not behave with the .fit method, instead .fit_generator must be used.\n",
    "\n",
    "Resolution explained in a follow-up question on the thread here: https://github.com/keras-team/keras/issues/6776\n",
    "\n",
    "Proper use example: https://datascience.stackexchange.com/questions/26366/training-an-rnn-with-examples-of-different-lengths-in-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Code - Sentiment Analysis\n",
    "\n",
    "Since this example is mainly for demonstrating Keras usage with variable sequence length, using a minimal model:\n",
    "\n",
    "    1) Just training our own embedding rather than using Glove or something smart.\n",
    "    2) One layer LSTM followed by a Dense layer, low values for hidden dim and embedding dim.\n",
    "    3) Use Bidirectional so the model isn't completely hopeless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/geugon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/geugon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Standard Stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, LSTM, Bidirectional, Embedding\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "def clean_text(orig_text):\n",
    "    \n",
    "    # html cleanup\n",
    "    soup = BeautifulSoup(orig_text, \"html.parser\")\n",
    "    review = soup.get_text()\n",
    "    \n",
    "    # white space cleanup\n",
    "    review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    review = review.lower().split()\n",
    "    \n",
    "    # lem and stopword removal \n",
    "    review = [lem.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data from:\n",
    "# https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/data?select=IMDB+Dataset.csv\n",
    "\n",
    "df = pd.read_csv(\"~/Downloads/IMDB_Dataset.csv\")\n",
    "raw_reviews = df['review'][:1000].apply(clean_text) #slow!\n",
    "sentiment = to_categorical(np.where(df['sentiment']=='positive', 1, 0))\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3516\n"
     ]
    }
   ],
   "source": [
    "index = np.arange(len(raw_reviews))\n",
    "np.random.shuffle(index)\n",
    "n_train = 900#49500\n",
    "n_valid = 100#500\n",
    "\n",
    "counts = Counter(raw_reviews[index[:n_train]].sum())\n",
    "id_to_token = [k for k, v in counts.items() if v>4]\n",
    "vocab_size = len(id_to_token)\n",
    "token_to_id = defaultdict(lambda : vocab_size, \n",
    "                   ((v,k) for k,v in enumerate(id_to_token)))\n",
    "id_to_token.append('<RARE>')\n",
    "\n",
    "reviews = [np.array([token_to_id[token] for token in review]) for review in raw_reviews]\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    while True:\n",
    "        for i in range(0, n_train):\n",
    "            x = reviews[index[i]].reshape(1,-1)\n",
    "            y = sentiment[index[i]].reshape(1,2)\n",
    "            yield x,y\n",
    "\n",
    "def valid_generator():\n",
    "    while True:\n",
    "        for i in range(n_train, n_train+n_valid):\n",
    "            x = reviews[index[i]].reshape(1,-1)\n",
    "            y = sentiment[index[i]].reshape(1,2)\n",
    "            yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0705 20:18:06.624911 139773621778176 deprecation_wrapper.py:119] From /home/geugon/anaconda3/envs/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0705 20:18:06.647186 139773621778176 deprecation_wrapper.py:119] From /home/geugon/anaconda3/envs/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0705 20:18:06.651402 139773621778176 deprecation_wrapper.py:119] From /home/geugon/anaconda3/envs/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0705 20:18:07.246141 139773621778176 deprecation_wrapper.py:119] From /home/geugon/anaconda3/envs/keras/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0705 20:18:07.271652 139773621778176 deprecation_wrapper.py:119] From /home/geugon/anaconda3/envs/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 16)          56272     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                12544     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 68,946\n",
      "Trainable params: 68,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "hidden_dim = 32\n",
    "\n",
    "input_ = Input(shape=(None,))\n",
    "embed = Embedding(vocab_size + 1, embedding_dim)(input_)\n",
    "rnn = Bidirectional(LSTM(hidden_dim, return_sequences=False))(embed)\n",
    "predict = Dense(2, activation='sigmoid')(rnn)\n",
    "model = Model(inputs=input_, outputs=predict)\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 20:18:07.453125 139773621778176 deprecation.py:323] From /home/geugon/anaconda3/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0705 20:18:08.939650 139773621778176 deprecation_wrapper.py:119] From /home/geugon/anaconda3/envs/keras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "900/900 [==============================] - 137s 153ms/step - loss: 0.6864 - acc: 0.5656 - val_loss: 0.5766 - val_acc: 0.7600\n",
      "Epoch 2/3\n",
      "900/900 [==============================] - 195s 217ms/step - loss: 0.4564 - acc: 0.7667 - val_loss: 0.5778 - val_acc: 0.7400\n",
      "Epoch 3/3\n",
      "900/900 [==============================] - 419s 466ms/step - loss: 0.1550 - acc: 0.9444 - val_loss: 0.8735 - val_acc: 0.6400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f45eb0da0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator(), \n",
    "                    validation_data=valid_generator(),\n",
    "                    steps_per_epoch = n_train, #batch size is inherently 1 via generator\n",
    "                    validation_steps= n_valid,\n",
    "                    epochs=3,\n",
    "                    verbose=1,)\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
