{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Models\n",
    "\n",
    "This notebook explores how to implement Markov models using basic Python syntax, starting with simplest possible form of this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Python Syntax\n",
    "#### Skip this section if you have a solid Python background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zip command allows for simultaneous interation through multiple objects.  The most typical use case is to loop through the contents of two lists at the same time.  Upon each step of the zip iteration, a tuple is returned.  This can be unpacked immediately (see the use of \"i, j\" below) or the entire tuple stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpack as we iterate\n",
      "1 4\n",
      "2 5\n",
      "3 9\n",
      "4 10\n",
      "\n",
      "just iterate\n",
      "(1, 4)\n",
      "(2, 5)\n",
      "(3, 9)\n",
      "(4, 10)\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "b = [4,5,9,10]\n",
    "\n",
    "print('unpack as we iterate')\n",
    "for i, j in zip(a,b):\n",
    "    print(i,j)\n",
    "    \n",
    "print(\"\\njust iterate\")\n",
    "for values in zip(a,b):\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zip command provides a handy way to iterate through a single object while grabbing multiple items.  This can be done by using the same input list for multiple argurments and slicing to trim them down.\n",
    "\n",
    "(Technical note: Your input object in this case must be an iterable that does not exhaust or you need to make a copy of it.  No worries for lists.)\n",
    "\n",
    "(Technical note 2: \"Better\" solutions for iterating through pairs of items exist.  Read more about itertools or better yet how generators work if you want a more robust formulation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'blue', 'green']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "b = ['red','blue','green','purple']\n",
    "\n",
    "b[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'blue', 'green']\n",
      "['blue', 'green', 'purple']\n",
      "\n",
      "red blue\n",
      "blue green\n",
      "green purple\n"
     ]
    }
   ],
   "source": [
    "print(b[:-1])\n",
    "print(b[1:])\n",
    "print()\n",
    "for prev, cur in zip(b[:-1], b[1:]):\n",
    "    print(prev, cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next we need to talk a little about details of the python dictionary.  A dictionary creates a mapping between pairs of keys and values.  Something new Python users don't immediately realize is that keys can be any immutable object.  In practical terms, this means that tuples are an excellent choice.  Additionally, the values themselves can be an arbitrariy complext object, such as another dictionary.  This enables more involved mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "bat\n",
      "{'solution to everything': 42}\n"
     ]
    }
   ],
   "source": [
    "simple_dict_0_0 = {1: 'a', 2: 'b'}\n",
    "simple_dict_1_0 = {'baseball': 'bat', 'tennis': 'racket'}\n",
    "simple_dict_0_1 = {}\n",
    "simple_dict_1_1 = {'solution to everything': 42}\n",
    "\n",
    "print(simple_dict_0_0[1])\n",
    "print(simple_dict_1_0['baseball'])\n",
    "print(simple_dict_1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {1: 'a', 2: 'b'},\n",
       " (0, 1): {},\n",
       " (1, 0): {'baseball': 'bat', 'tennis': 'racket'},\n",
       " (1, 1): {'solution to everything': 42}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = {}\n",
    "my_dict[(0,0)] = simple_dict_0_0\n",
    "my_dict[(0,1)] = simple_dict_0_1\n",
    "my_dict[(1,0)] = simple_dict_1_0\n",
    "my_dict[(1,1)] = simple_dict_1_1\n",
    "\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access a sub-dictionary for my_dict using []\n",
      "{'solution to everything': 42}\n",
      "\n",
      "Access a single value of taht sub-dictionary for my_dict using [] twice in a row\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "print('Access a sub-dictionary for my_dict using []')\n",
    "print(my_dict[(1,1)])\n",
    "\n",
    "print('\\nAccess a single value of taht sub-dictionary for my_dict using [] twice in a row')\n",
    "print(my_dict[(1,1)]['solution to everything'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the cells above that we create a dictionary whose values are other dictionaries.  In this case, looking up a key with [ ] syntax returns another dictionary, in which another lookup using [ ] can be used immedately.  An example is shown in the last line of the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a system which is described at each instance by a state, and this state transitions to different states.  A classic example is the weather.  The current state could be described by temperature, windspeed, humity, and a number of other parameters.  Tomorrow, the system will have a different state.\n",
    "\n",
    "A Markov process in one in which the transition probabability of going to the next state is assumed to be entirely described by information in the current state.  Consider predicting tomorrow's weather.  If we model it as a Markov process, we are saying that information about today's weather will drive our prediction, but information about the yesterday's weather provides no additional information.\n",
    "\n",
    "We could apply such a model to describe English sentances, where the probability of a word appearing is assumed to be only a function of the word before it.  This is a very crude approximation for how English syntax, but it gives us a starting place for a simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text processing is a rich field unto itself, but for moment we want to focus on the mechanics of Markov models.  As such, will will start with a simple copy-paste of some text (in this case from the Wikipedia page on fire), and do minimal pre-processing.  The result is a list of words, where '.' is treated as a word.  Effectively we will be using '.' as SOS and EOS token, but let's gloss over those details for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_text = \"Fire is the rapid oxidation of a material in the exothermic chemical process of combustion, releasing heat, light, and various reaction products.[1] Slower oxidative processes like rusting or digestion are not included by this definition.  Fire is hot because the conversion of the weak double bond in molecular oxygen, O2, to the stronger bonds in the combustion products carbon dioxide and water releases energy (418 kJ per 32 g of O2); the bond energies of the fuel play only a minor role here.[2] At a certain point in the combustion reaction, called the ignition point, flames are produced. The flame is the visible portion of the fire. Flames consist primarily of carbon dioxide, water vapor, oxygen and nitrogen. If hot enough, the gases may become ionized to produce plasma.[3] Depending on the substances alight, and any impurities outside, the color of the flame and the fire's intensity will be different. Fire in its most common form can result in conflagration, which has the potential to cause physical damage through burning. Fire is an important process that affects ecological systems around the globe. The positive effects of fire include stimulating growth and maintaining various ecological systems. The negative effects of fire include hazard to life and property, atmospheric pollution, and water contamination.[4] If fire removes protective vegetation, heavy rainfall may lead to an increase in soil erosion by water.[5] Also, when vegetation is burned, the nitrogen it contains is released into the atmosphere, unlike elements such as potassium and phosphorus which remain in the ash and are quickly recycled into the soil. This loss of nitrogen caused by a fire produces a long-term reduction in the fertility of the soil, which only slowly recovers as nitrogen is fixed from the atmosphere by lightning and by leguminous plants such as clover. Fire has been used by humans in rituals, in agriculture for clearing land, for cooking, generating heat and light, for signaling, propulsion purposes, smelting, forging, incineration of waste, cremation, and as a weapon or mode of destruction.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = ['.'] + raw_text.lower().replace(\".\", \" . \").split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start building the model, it is informative to consider the simplier task counting how many times each word appears.  Below are two cells each performing the same task.  The first uses a dictionary and the second uses a defaultdict.\n",
    "\n",
    "While both datastructures give the same result, the latter has the advantage of assuming any new key maps to a value of zero.  This elimnates the need to check if we have already encountered a key.  As we will see, default dict also makes it convenient to built more complicated structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How man times does the word 'the' appear?\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "word_count = {}\n",
    "for word in text:\n",
    "    if word not in word_count.keys():\n",
    "        word_count[word] = 1\n",
    "    else:\n",
    "        word_count[word] += 1 # same as: word_count[word] = word_count[word] + 1 \n",
    "\n",
    "print(\"How man times does the word 'the' appear?\")        \n",
    "print(word_count['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How man times does the word 'the' appear?\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "word_count = defaultdict(int)\n",
    "for word in text:\n",
    "    word_count[word] += 1\n",
    "\n",
    "print(\"How man times does the word 'the' appear?\")     \n",
    "print(word_count['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Markov Model needs to represent the possiblity of each word transitioning to another word.  One could imagine this as a large matrix of probabilities, however, most word-word transitions never occur (at least not in our training input).  For example, you do not expect to see an adjective followed by an article such as 'chemical' -> 'the'.  Rather than use a matrix of mostly zeros, we are going to use a defaultdict to store possible transitions.\n",
    "\n",
    "(Technical note: scipy.sparse provides an implementation of sparse matricies which would an excellent storage choice for this problem.  This problem could also be seen as a graph creation and other packages such as networkx would be excellent way to convert word pairs into a directed graph.  However, this notebook is focused using basic python functionality.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transitions will be stored in a defaultdict, where each key is a single word and the each value is a list of words come after it.  This is NOT the best structure for the final model, but is a convenient choice for a short input text where we want to see some of the inner workings of the process.  We will switch to a better representation later on.\n",
    "\n",
    "For tracking the possible next words, the default value in our dictionary will be a list.  Whereas for counting we used '+= 1' to increment an int, this time we append to a list of words that have been observed to come after the left word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'is', '.', 'in', 'is', 'include', 'include', 'removes', 'produces', 'has']\n"
     ]
    }
   ],
   "source": [
    "observed_trans = defaultdict(list)\n",
    "for left, right in zip(text[:-1], text[1:]):\n",
    "    observed_trans[left].append(right)\n",
    "\n",
    "print(observed_trans['fire'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obsereved_tran['fire'] tells us which words came after 'fire' in our training text.  The pair \"fire is\" occured multiple times in the text, so 'is' is listed multiple times in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fire', '[1]', 'fire', '[2]', 'the', 'flames', 'if', '[3]', 'fire', 'fire', 'the', 'the', '[4]', '[5]', 'this', 'fire']\n"
     ]
    }
   ],
   "source": [
    "print(observed_trans['.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that '.' is sometimes followed by words like '[1]' because we did nothing to clean up citation references in the original text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what transitions occur, and how often, we can start doing some interesting things like sampling the distrubution to generate new sentances.  This is done by picking '.' as our starting word, randomly choosing a word that could follow it, printing that word, and then restarting the process with the new current word.  We stop when we enconter another period.\n",
    "\n",
    "Try running the following cell multiple times to see what all outputs it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "loss\n",
      "of\n",
      "nitrogen\n",
      "it\n",
      "contains\n",
      "is\n",
      "burned,\n",
      "the\n",
      "bond\n",
      "energies\n",
      "of\n",
      "combustion,\n",
      "releasing\n",
      "heat,\n",
      "light,\n",
      "and\n",
      "water\n",
      "contamination\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "cur = '.'\n",
    "\n",
    "for _ in range(30):\n",
    "    possible = observed_trans[cur]\n",
    "    cur = possible[randint(0,len(possible)-1)]\n",
    "    print(cur)\n",
    "    if cur=='.':\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The output is mostly a strangly meandering 'sentence' that almost makes sence at times, but is not quite right.  This is because the core assumption of the model (that the probaility of each word occuring is solely a function of the previous word) is flawed.  We will work to show how we can improve this model with additional complexity.  \n",
    "\n",
    "However, even though this model not sufficient to generate good new language, it has some potential use.  For example, it can tell us how reasonable an existing sentance is.  In the simplest form, we could see how likely a new sentance is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text_a = \"fire is the fire.\"\n",
    "raw_text_b = \"The the the the.\"\n",
    "\n",
    "# We have to apply the same text preprocessing to test text as we did our original training data\n",
    "test_text_a = ['.'] + raw_text_a.lower().replace(\".\", \" . \").split()\n",
    "test_text_b = ['.'] + raw_text_b.lower().replace(\".\", \" . \").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition: '.' to 'fire' has observed probability 0.3125\n",
      "Transition: 'fire' to 'is' has observed probability 0.3\n",
      "Transition: 'is' to 'the' has observed probability 0.2857142857142857\n",
      "Transition: 'the' to 'fire' has observed probability 0.034482758620689655\n",
      "Transition: 'fire' to '.' has observed probability 0.1\n",
      "Total prob of sentance: 9.236453201970444e-05\n"
     ]
    }
   ],
   "source": [
    "# test a\n",
    "cum_prob = 1\n",
    "for left, right in zip(test_text_a[:-1], test_text_a[1:]):\n",
    "    prob = observed_trans[left].count(right) / len(observed_trans[left])\n",
    "    cum_prob *= prob\n",
    "    print(\"Transition: '{}' to '{}' has observed probability {}\".format(left, right, prob))\n",
    "print(\"Total prob of sentance:\", cum_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition: '.' to 'the' has observed probability 0.1875\n",
      "Transition: 'the' to 'the' has observed probability 0.0\n",
      "Transition: 'the' to 'the' has observed probability 0.0\n",
      "Transition: 'the' to 'the' has observed probability 0.0\n",
      "Transition: 'the' to '.' has observed probability 0.0\n",
      "Total prob of sentance: 0.0\n"
     ]
    }
   ],
   "source": [
    "# test b\n",
    "cum_prob = 1\n",
    "for left, right in zip(test_text_b[:-1], test_text_b[1:]):\n",
    "    prob = observed_trans[left].count(right) / len(observed_trans[left])\n",
    "    cum_prob *= prob\n",
    "    print(\"Transition: '{}' to '{}' has observed probability {}\".format(left, right, prob))\n",
    "print(\"Total prob of sentance:\", cum_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples, the probability of transition was assumed to be the same a the fraction of time the same transition was observed in training data.  For example 'is' occured 3 times after the word 'fire' out of the time times 'fire' existed in the training text, so the probability was 0.3 or 30%.  The probability of an entire sentance was then assumed to be the product of all the individual word transitions.\n",
    "\n",
    "This is roughly correct, but has a serious shortcomming.  It assumes that anything transition not observed is impossible.  For this reason, a good model needs to use smoothing (or an alternative technique) to deal with the fact that the training set is not a complete description of all possible transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "From here, there are several pathways to explore for improving the model.  First, smoothing is vital to deal with overfitting by the model.  Second, this was a true \"Markov Model\", rather than a \"Hidden Markov Model\", so extending the model to have both transition and emission probabilities is necessary to create a more robust tool.  Third, our model only used the previous word for transitions, not the previous two words, or three.  Extending the definition of the state can allow the model to more realistically mimic English syntax.  However, techniques like smoothing become even more vital or the model will just overfit the training data more.\n",
    "\n",
    "Beyond the model itself, the way we used it to describe and generate new sentances was crude.  Once a better model is created based on the methods of the previous paragraph, Vitterbi and other algorithms provide smarter ways of leveraging the model.\n",
    "\n",
    "This notebook provided basic overview and samples of using core Python tools to dig into a real Machine Learning problem, and providing an hands-on sample of what Markov processes are about.  A future notebook will explore the NLP problem more deeply."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
