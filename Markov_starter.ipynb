{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Models\n",
    "\n",
    "This notebook explores how to implement Markov models using basic Python syntax, starting with simplest possible form of this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Python Syntax\n",
    "#### Skip this section if you have a solid Python background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zip command allows for simultaneous interation through multiple objects.  The most typical use case is to loop through the contents of two lists at the same time.  Upon each step of the zip iteration, a tuple is returned.  This can be unpacked immediately (see the use of \"i, j\" below) or the entire tuple stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpack as we iterate\n",
      "1 4\n",
      "2 5\n",
      "3 9\n",
      "4 10\n",
      "\n",
      "just iterate\n",
      "(1, 4)\n",
      "(2, 5)\n",
      "(3, 9)\n",
      "(4, 10)\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "b = [4,5,9,10]\n",
    "\n",
    "print('unpack as we iterate')\n",
    "for i, j in zip(a,b):\n",
    "    print(i,j)\n",
    "    \n",
    "print(\"\\njust iterate\")\n",
    "for values in zip(a,b):\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zip command provides a handy way to iterate through a single object while grabbing multiple items.  This can be done by using the same input list for multiple argurments and slicing to trim them down.\n",
    "\n",
    "(Technical note: Your input object in this case must be an iterable that does not exhaust or you need to make a copy of it.  No worries for lists.)\n",
    "\n",
    "(Technical note 2: \"Better\" solutions for iterating through pairs of items exist.  Read more about itertools or better yet how generators work if you want a more robust formulation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[2, 3, 4]\n",
      "\n",
      "1 2\n",
      "2 3\n",
      "3 4\n"
     ]
    }
   ],
   "source": [
    "print(a[:-1])\n",
    "print(a[1:])\n",
    "print()\n",
    "for prev, cur in zip(a[:-1], a[1:]):\n",
    "    print(prev, cur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next we need to talk a little about details of the python dictionary.  A dictionary creates a mapping between pairs of keys and values.  Something new Python users don't immediately realize is that keys can be any immutable object.  In practical terms, this means that tuples are an excellent choice.  Additionally, the values themselves can be an arbitrariy complext object, such as another dictionary.  This enables more involved mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "bat\n",
      "{'solution to everything': 42}\n"
     ]
    }
   ],
   "source": [
    "simple_dict_0_0 = {1: 'a', 2: 'b'}\n",
    "simple_dict_1_0 = {'baseball': 'bat', 'tennis': 'racket'}\n",
    "simple_dict_0_1 = {}\n",
    "simple_dict_1_1 = {'solution to everything': 42}\n",
    "\n",
    "print(simple_dict_0_0[1])\n",
    "print(simple_dict_1_0['baseball'])\n",
    "print(simple_dict_1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = {}\n",
    "my_dict[(0,0)] = simple_dict_0_0\n",
    "my_dict[(0,1)] = simple_dict_0_1\n",
    "my_dict[(1,0)] = simple_dict_1_0\n",
    "my_dict[(1,1)] = simple_dict_1_1\n",
    "\n",
    "my_dict[(1,1)]['solution to everything']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the cell above that we create a dictionary whose values are other dictionaries.  In this case, looking up a key with [ ] syntax returns another dictionary, in which another lookup using [ ] can be used immedately.  An example is shown in the last line of the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a system which is described at each instance by a state, and this state transitions to different states.  A classic example is the weather.  The current state could be described by temperature, windspeed, humity, and a number of other parameters.  Tomorrow, the system will have a different state.\n",
    "\n",
    "A Markov process in one in which the transition probabability of going to the next state is assumed to be entired described by information about the current state.  Consider predicting tomorrow's weather.  If we model it as a Markov process, we are saying that information about today's weather will drive our prediction, but information about the yesterday's weather provides no additional information.\n",
    "\n",
    "We could apply such a model to describe English sentances, where the probability of a word appearing is assumed to be only a function of the word before it.  This is a very crude approximation for how English sentaces are structured, but it gives us a starting place for a simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text processing is a rich field unto itself, but for moment we want to focus on the mechanics of Markov models.  As such, will will start with a simple copy-paste of some text (in this case from the Wikipedia page on fire), and do minimal pre-processing.  The result is a list of words, where '.' is treated as a word.  Effectively we will be using '.' as SOS and EOS token, but let's gloss over those details for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_text = \"Fire is the rapid oxidation of a material in the exothermic chemical process of combustion, releasing heat, light, and various reaction products.[1] Slower oxidative processes like rusting or digestion are not included by this definition.  Fire is hot because the conversion of the weak double bond in molecular oxygen, O2, to the stronger bonds in the combustion products carbon dioxide and water releases energy (418 kJ per 32 g of O2); the bond energies of the fuel play only a minor role here.[2] At a certain point in the combustion reaction, called the ignition point, flames are produced. The flame is the visible portion of the fire. Flames consist primarily of carbon dioxide, water vapor, oxygen and nitrogen. If hot enough, the gases may become ionized to produce plasma.[3] Depending on the substances alight, and any impurities outside, the color of the flame and the fire's intensity will be different. Fire in its most common form can result in conflagration, which has the potential to cause physical damage through burning. Fire is an important process that affects ecological systems around the globe. The positive effects of fire include stimulating growth and maintaining various ecological systems. The negative effects of fire include hazard to life and property, atmospheric pollution, and water contamination.[4] If fire removes protective vegetation, heavy rainfall may lead to an increase in soil erosion by water.[5] Also, when vegetation is burned, the nitrogen it contains is released into the atmosphere, unlike elements such as potassium and phosphorus which remain in the ash and are quickly recycled into the soil. This loss of nitrogen caused by a fire produces a long-term reduction in the fertility of the soil, which only slowly recovers as nitrogen is fixed from the atmosphere by lightning and by leguminous plants such as clover. Fire has been used by humans in rituals, in agriculture for clearing land, for cooking, generating heat and light, for signaling, propulsion purposes, smelting, forging, incineration of waste, cremation, and as a weapon or mode of destruction.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = ['.'] + raw_text.lower().replace(\".\", \" . \").split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start building the model, it is informative to consider the simplier task counting how many times each word appears.  Below are two cells each performing the same task.  The first uses a dictionary and the second uses a defaultdict.\n",
    "\n",
    "While both datastructures give the same result, the latter has the advantage of assuming any new key maps to a value of zero.  This elimnates the need to check if we have already encountered a key.  As we will see, default dict also makes it convenient to built more complicated structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "word_count = {}\n",
    "for word in text:\n",
    "    if word not in word_count.keys():\n",
    "        word_count[word] = 1\n",
    "    else:\n",
    "        word_count[word] += 1\n",
    "        \n",
    "print(word_count['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "word_count = defaultdict(int)\n",
    "for word in text:\n",
    "    word_count[word] += 1\n",
    "print(word_count['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Markov Model needs to represent the possiblity of each word transitioning to another word.  One could imagine this as a large matrix of probabilities, however, most word-word transitions never occur (at least not in our training input).  For example, you do not expect to see an adjective followed by an article such as 'chemical' -> 'the'.  Rather than use a matrix of mostly zeros, we are going to use a defaultdict to store possible transitions.\n",
    "\n",
    "(Technical note: scipy.sparse provides an implementation of sparse matricies which would an excellent storage choice for this problem.  However, this notebook is focused using basic python functionality.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transitions will be stored in a defaultdict, where each key is a single word and the each value is a list of words come after it.  This is NOT the best structure for the final model, but is a convenient choice for a short input text where we want to see some of the inner workings of the process.  We will switch to a better representation later on.\n",
    "\n",
    "This time the default value is a list.  Whereas for counting we used '+= 1' to increment an int, this time we append to a list of words that have been observed to come after the left word.\n",
    "\n",
    "The obsereved_tran['fire'] tell us which words can come after 'fire' including repeats.  Note that '.' is sometimes followed by words like '[1]' because we did nothing to clean those from the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'is', '.', 'in', 'is', 'include', 'include', 'removes', 'produces', 'has']\n",
      "['fire', '[1]', 'fire', '[2]', 'the', 'flames', 'if', '[3]', 'fire', 'fire', 'the', 'the', '[4]', '[5]', 'this', 'fire']\n"
     ]
    }
   ],
   "source": [
    "observed_trans = defaultdict(list)\n",
    "for left, right in zip(text[:-1], text[1:]):\n",
    "    observed_trans[left].append(right)\n",
    "print(observed_trans['fire'])\n",
    "print(observed_trans['.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what transitions occur, and how often, we can start doing some interesting things like sampling the distrubution to generate new sentances.  This is done by picking '.' as our starting word, randomly choosing word that could follow it, printing that word, and then restarting the process with the new current word.  We stop when we enconter another period.\n",
    "\n",
    "Try running the following cell multiple times to see what all outputs it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flames\n",
      "are\n",
      "quickly\n",
      "recycled\n",
      "into\n",
      "the\n",
      "stronger\n",
      "bonds\n",
      "in\n",
      "conflagration,\n",
      "which\n",
      "remain\n",
      "in\n",
      "rituals,\n",
      "in\n",
      "agriculture\n",
      "for\n",
      "signaling,\n",
      "propulsion\n",
      "purposes,\n",
      "smelting,\n",
      "forging,\n",
      "incineration\n",
      "of\n",
      "the\n",
      "globe\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "cur = '.'\n",
    "\n",
    "for _ in range(30):\n",
    "    possible = observed_trans[cur]\n",
    "    cur = possible[randint(0,len(possible)-1)]\n",
    "    print(cur)\n",
    "    if cur=='.':\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The output is mostly a strangly meandering 'sentence' that almost makes sence at times, but is not quite right.  This is because the core assumption of the model (that the probaility of each word occuring is solely a function of the previous word) is flawed.  We will work to show how we can improve this model with additional complexity.  \n",
    "\n",
    "However, even though this model not sufficient to generate good new language, it has some potential use...\n",
    "\n",
    "TODO...show how we can detect gibberish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
